{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Sigmoid Perceptron from Scratch\n","Esta jupyter notebook implementa un modelo de perceptrón con función de activación sigmoide para la clasificación binaria desde cero (sin usar las funciones que existen en algunas librerías).\n","\n","Nota: Cuando digo salida directa, me refiero a una salida visible en el notebook."]},{"cell_type":"markdown","metadata":{},"source":["## 1. Instalación e Importación de librerías\n","\n","Este bloque de código instala e importa las bibliotecas necesarias para la obtención, manipulación, procesamiento, entrenamiento y visualización de datos, que serán utilizadas en la implementación del modelo de perceptrón sigmoide para la clasificación binaria. Las bibliotecas incluyen herramientas para manejar datos, crear gráficos, realizar cálculos numéricos, y usar redes neuronales con TensorFlow."]},{"cell_type":"markdown","metadata":{},"source":["**Salida**: No hay salida directa, solo se importan las librerías necesarias para el resto del notebook."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37019,"status":"ok","timestamp":1726229336734,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"REyK8cG6Qisu","outputId":"f2c6ce0d-0539-4e44-ca3d-274be2288b27"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: ucimlrepo in c:\\users\\rodrigo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.7)\n","Requirement already satisfied: pandas>=1.0.0 in c:\\users\\rodrigo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ucimlrepo) (2.2.1)\n","Requirement already satisfied: certifi>=2020.12.5 in c:\\users\\rodrigo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ucimlrepo) (2024.2.2)\n","Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\rodrigo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rodrigo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\rodrigo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rodrigo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n","Requirement already satisfied: six>=1.5 in c:\\users\\rodrigo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# Instalamos la librería 'ucimlrepo' para acceder al repositorio de datasets de UCI\n","%pip install ucimlrepo  \n","\n","# Importamos la función 'fetch_ucirepo' para descargar datasets de UCI\n","from ucimlrepo import fetch_ucirepo  \n","# Importamos pandas para manejar datos en forma de DataFrames\n","import pandas as pd  \n","# Importamos matplotlib para visualizar los datos mediante gráficos\n","import matplotlib.pyplot as plt  \n","# Importamos numpy para realizar cálculos matemáticos y manejar arrays\n","import numpy as np  \n","# Importamos TensorFlow para crear y entrenar modelos de redes neuronales\n","import tensorflow as tf  \n","# Importamos 'train_test_split' para dividir los datos en entrenamiento y prueba\n","from sklearn.model_selection import train_test_split  \n","# Importamos 'StandardScaler' para normalizar los datos y escalar las características\n","from sklearn.preprocessing import StandardScaler  "]},{"cell_type":"markdown","metadata":{},"source":["## 2. Descarga y Preparación de los Datos\n","\n","En este bloque se descarga el dataset \"`Occupancy Detection`\" desde el repositorio UCI utilizando su ID, y se almacena en una variable. A continuación, se extraen los datos originales para ser procesados en las siguientes etapas del modelo de perceptrón sigmoide."]},{"cell_type":"markdown","metadata":{},"source":["**Salida**: No hay salida directa. El dataset \"Occupancy Detection\" se descarga y se almacena en la variable data para su posterior procesamiento."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2040,"status":"ok","timestamp":1726229340831,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"VR0P_ZhYQisw"},"outputs":[],"source":["# Download `occupancy-detection` dataset\n","# using the id=357\n","# Descargar el dataset 'Occupancy Detection' usando su identificador en el repositorio UCI (ID=357)\n","occupancy_detection = fetch_ucirepo(id=357) \n","\n","# Build the feature and the target data sets\n","# Extraer los datos originales del dataset descargado y almacenarlos en la variable 'data'\n","data = occupancy_detection.data.original "]},{"cell_type":"markdown","metadata":{},"source":["## 3. Limpieza y Transformación de Datos\n","Este bloque elimina las columnas innecesarias del dataset, como las columnas de `date` e `id`, que no aportan valor al modelo. Posteriormente, se asegura de que todas las columnas contengan datos numéricos y elimina las filas que tengan valores faltantes o incorrectos."]},{"cell_type":"markdown","metadata":{},"source":["**Salida**: No hay salida directa. Se realiza la limpieza de datos, eliminando las columnas irrelevantes (`date` e `id`), convirtiendo los valores a tipo numérico, y eliminando filas con valores faltantes (NaN), preparando los datos para su uso en el modelo."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":666,"status":"ok","timestamp":1726229343639,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"rew6AD7bQisw"},"outputs":[],"source":["# Eliminamos las columnas 'date' e 'id' ya que no son útiles para el modelo\n","df = data.drop(['date', 'id'], axis=1)\n","\n","# Convertimos todos los valores de las columnas a tipo numérico, forzando a NaN aquellos que no se puedan convertir\n","df[df.columns] = df[df.columns].apply(pd.to_numeric, errors='coerce')\n","\n","# Eliminamos las filas que contengan valores NaN generados por la conversión o por datos faltantes\n","df = df.dropna()"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Visualización del DataFrame\n","Este bloque simplemente visualiza el contenido del DataFrame `df` después de la limpieza y transformación de los datos. Nos permite observar la estructura y las características del dataset, asegurándonos de que todo esté correctamente formateado antes de proceder con el análisis."]},{"cell_type":"markdown","metadata":{},"source":["**Salida**: Muestra el contenido del DataFrame `df` después de la limpieza, incluyendo las características y la variable objetivo (`Occupancy`). Se puede observar la estructura del dataset con las filas y columnas resultantes tras el preprocesamiento."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"gfl5mAE1Qisw","outputId":"884f7299-2296-4c7a-ed22-05f11cff96f1"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Temperature</th>\n","      <th>Humidity</th>\n","      <th>Light</th>\n","      <th>CO2</th>\n","      <th>HumidityRatio</th>\n","      <th>Occupancy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>23.180</td>\n","      <td>27.2720</td>\n","      <td>426.00</td>\n","      <td>721.25</td>\n","      <td>0.004793</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>23.150</td>\n","      <td>27.2675</td>\n","      <td>429.50</td>\n","      <td>714.00</td>\n","      <td>0.004783</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>23.150</td>\n","      <td>27.2450</td>\n","      <td>426.00</td>\n","      <td>713.50</td>\n","      <td>0.004779</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>23.150</td>\n","      <td>27.2000</td>\n","      <td>426.00</td>\n","      <td>708.25</td>\n","      <td>0.004772</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>23.100</td>\n","      <td>27.2000</td>\n","      <td>426.00</td>\n","      <td>704.50</td>\n","      <td>0.004757</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>20557</th>\n","      <td>20.815</td>\n","      <td>27.7175</td>\n","      <td>429.75</td>\n","      <td>1505.25</td>\n","      <td>0.004213</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>20558</th>\n","      <td>20.865</td>\n","      <td>27.7450</td>\n","      <td>423.50</td>\n","      <td>1514.50</td>\n","      <td>0.004230</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>20559</th>\n","      <td>20.890</td>\n","      <td>27.7450</td>\n","      <td>423.50</td>\n","      <td>1521.50</td>\n","      <td>0.004237</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>20560</th>\n","      <td>20.890</td>\n","      <td>28.0225</td>\n","      <td>418.75</td>\n","      <td>1632.00</td>\n","      <td>0.004279</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>20561</th>\n","      <td>21.000</td>\n","      <td>28.1000</td>\n","      <td>409.00</td>\n","      <td>1864.00</td>\n","      <td>0.004321</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20560 rows × 6 columns</p>\n","</div>"],"text/plain":["       Temperature  Humidity   Light      CO2  HumidityRatio  Occupancy\n","0           23.180   27.2720  426.00   721.25       0.004793        1.0\n","1           23.150   27.2675  429.50   714.00       0.004783        1.0\n","2           23.150   27.2450  426.00   713.50       0.004779        1.0\n","3           23.150   27.2000  426.00   708.25       0.004772        1.0\n","4           23.100   27.2000  426.00   704.50       0.004757        1.0\n","...            ...       ...     ...      ...            ...        ...\n","20557       20.815   27.7175  429.75  1505.25       0.004213        1.0\n","20558       20.865   27.7450  423.50  1514.50       0.004230        1.0\n","20559       20.890   27.7450  423.50  1521.50       0.004237        1.0\n","20560       20.890   28.0225  418.75  1632.00       0.004279        1.0\n","20561       21.000   28.1000  409.00  1864.00       0.004321        1.0\n","\n","[20560 rows x 6 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Mostramos el contenido del DataFrame 'df' para verificar su estructura después de la limpieza\n","df"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Descripción Estadística de los Datos\n","Este bloque genera un resumen estadístico de las características numéricas del DataFrame. Muestra información como el promedio, la desviación estándar, los valores mínimo y máximo, así como los percentiles, lo que ayuda a entender mejor la distribución de los datos."]},{"cell_type":"markdown","metadata":{},"source":["**Salida**: Se genera un resumen estadístico del DataFrame `df`, que incluye métricas como la media, desviación estándar, valores mínimos, máximos y percentiles (25%, 50%, 75%) de cada característica del conjunto de datos."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"elapsed":562,"status":"ok","timestamp":1726229347094,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"ELCVREAKQisw","outputId":"cce4a981-4706-406f-f635-fdfd47f54eeb"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Temperature</th>\n","      <th>Humidity</th>\n","      <th>Light</th>\n","      <th>CO2</th>\n","      <th>HumidityRatio</th>\n","      <th>Occupancy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>20560.000000</td>\n","      <td>20560.000000</td>\n","      <td>20560.000000</td>\n","      <td>20560.000000</td>\n","      <td>20560.000000</td>\n","      <td>20560.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>20.906212</td>\n","      <td>27.655925</td>\n","      <td>130.756622</td>\n","      <td>690.553276</td>\n","      <td>0.004228</td>\n","      <td>0.231031</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>1.055315</td>\n","      <td>4.982154</td>\n","      <td>210.430875</td>\n","      <td>311.201281</td>\n","      <td>0.000768</td>\n","      <td>0.421503</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>19.000000</td>\n","      <td>16.745000</td>\n","      <td>0.000000</td>\n","      <td>412.750000</td>\n","      <td>0.002674</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>20.200000</td>\n","      <td>24.500000</td>\n","      <td>0.000000</td>\n","      <td>460.000000</td>\n","      <td>0.003719</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>20.700000</td>\n","      <td>27.290000</td>\n","      <td>0.000000</td>\n","      <td>565.416667</td>\n","      <td>0.004292</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>21.525000</td>\n","      <td>31.290000</td>\n","      <td>301.000000</td>\n","      <td>804.666667</td>\n","      <td>0.004832</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>24.408333</td>\n","      <td>39.500000</td>\n","      <td>1697.250000</td>\n","      <td>2076.500000</td>\n","      <td>0.006476</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        Temperature      Humidity         Light           CO2  HumidityRatio  \\\n","count  20560.000000  20560.000000  20560.000000  20560.000000   20560.000000   \n","mean      20.906212     27.655925    130.756622    690.553276       0.004228   \n","std        1.055315      4.982154    210.430875    311.201281       0.000768   \n","min       19.000000     16.745000      0.000000    412.750000       0.002674   \n","25%       20.200000     24.500000      0.000000    460.000000       0.003719   \n","50%       20.700000     27.290000      0.000000    565.416667       0.004292   \n","75%       21.525000     31.290000    301.000000    804.666667       0.004832   \n","max       24.408333     39.500000   1697.250000   2076.500000       0.006476   \n","\n","          Occupancy  \n","count  20560.000000  \n","mean       0.231031  \n","std        0.421503  \n","min        0.000000  \n","25%        0.000000  \n","50%        0.000000  \n","75%        0.000000  \n","max        1.000000  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Generamos un resumen estadístico del DataFrame, incluyendo métricas como media, desviación estándar, y percentiles\n","df.describe()"]},{"cell_type":"markdown","metadata":{},"source":["## 6. Establecimiento de la Semilla Aleatoria\n","Este bloque de código establece una semilla aleatoria para TensorFlow, lo cual asegura que los resultados del modelo sean reproducibles. Es útil cuando se trabaja con redes neuronales para que los valores iniciales generados aleatoriamente sean los mismos en cada ejecución del código."]},{"cell_type":"markdown","metadata":{},"source":["**Salida**: No hay salida directa. Se establece una semilla aleatoria para garantizar la reproducibilidad de los resultados en TensorFlow, asegurando que los valores aleatorios generados en el modelo sean los mismos en cada ejecución."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":381,"status":"ok","timestamp":1726229349123,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"_qmL27_KQisw"},"outputs":[],"source":["# Establecemos una semilla aleatoria para garantizar la reproducibilidad de los resultados en TensorFlow\n","tf.random.set_seed(4500)"]},{"cell_type":"markdown","metadata":{},"source":["## 7. Separación de Características y Variable Objetivo\n","En este bloque, se separan las características (X) y la variable objetivo (y) del DataFrame. La columna `Occupancy` se establece como la variable objetivo (y), mientras que el resto de las columnas se usan como características (X) para entrenar el modelo."]},{"cell_type":"markdown","metadata":{},"source":["**Salida**: No hay salida directa. Se separan las características (almacenadas en `X`) y la variable objetivo (`y`) del DataFrame. Las características se utilizan para entrenar el modelo, mientras que la variable objetivo contiene las etiquetas de clasificación."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":417,"status":"ok","timestamp":1726229350763,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"4sIFY_kAQisw"},"outputs":[],"source":["# Separar las características (variables independientes) eliminando la columna 'Occupancy' que es la variable objetivo\n","X = df.drop([\"Occupancy\"], axis=1)\n","\n","# Asignar la columna 'Occupancy' como la variable objetivo\n","y = df['Occupancy']"]},{"cell_type":"markdown","metadata":{},"source":["## 8. División del Conjunto de Datos en Entrenamiento y Prueba\n","Este bloque divide el dataset en dos partes: un conjunto de entrenamiento y un conjunto de prueba. Se asigna el 80% de los datos para el entrenamiento del modelo y el 20% para su evaluación. La opción `shuffle=True` asegura que los datos se mezclen antes de dividirse para evitar sesgos, y se establece una semilla de aleatorización (`random_state=42`) para garantizar la reproducibilidad."]},{"cell_type":"markdown","metadata":{},"source":["**Salida**: No hay salida directa. El conjunto de datos se divide en conjuntos de entrenamiento (`X_train`, `y_train`) y de prueba (`X_test`, `y_test`), asignando el 80% de los datos para entrenamiento y el 20% para evaluación, con los datos mezclados de manera aleatoria."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":305,"status":"ok","timestamp":1726229352060,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"6yPyLEblQisw"},"outputs":[],"source":["# Dividimos los datos en conjuntos de entrenamiento (80%) y prueba (20%)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["## 9. Normalización de las Características\n","Este bloque aplica la normalización a las características (X) utilizando `StandardScaler`. Este proceso ajusta las características para que tengan una media de 0 y una desviación estándar de 1, lo cual mejora el rendimiento del modelo al asegurar que todas las características estén en la misma escala."]},{"cell_type":"markdown","metadata":{},"source":["**Salida**: No hay salida directa. Las características de los conjuntos de entrenamiento y prueba se estandarizan utilizando `StandardScaler`, ajustando las características para que tengan una media de 0 y una desviación estándar de 1. Esto asegura que todas las características estén en la misma escala."]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1726229353635,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"O7J3VULnQisx"},"outputs":[],"source":["# Inicializamos el objeto 'StandardScaler' para estandarizar las características\n","scaler = StandardScaler()\n","\n","# Ajustamos y transformamos el conjunto de entrenamiento, estandarizando las características\n","X_train = scaler.fit_transform(X_train)\n","\n","# Transformamos el conjunto de prueba utilizando los mismos parámetros obtenidos del conjunto de entrenamiento\n","X_test = scaler.transform(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["## 10. Implementación del Modelo de Neurona (Perceptrón Sigmoide)\n","En este bloque se define una clase `NeuronModel` que implementa un perceptrón sigmoide para la clasificación binaria. La clase incluye los métodos para inicializar el modelo, realizar la propagación hacia adelante y atrás, actualizar los parámetros, y entrenar el modelo. También se incluyen métodos para evaluar el modelo, predecir resultados y visualizar la función de costo"]},{"cell_type":"markdown","metadata":{},"source":["**Salida**: No hay salida directa. Se define la clase `NeuronModel`, que implementa un perceptrón sigmoide desde cero, incluyendo funciones para la propagación hacia adelante, la propagación hacia atrás, el cálculo de la función de costo, la actualización de los parámetros, la predicción y la evaluación del modelo. Esta clase se utilizará para entrenar y evaluar el modelo."]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":332,"status":"ok","timestamp":1726229356530,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"sxdZAxWxQisx"},"outputs":[],"source":["# Importamos la función expit, que es la versión de scipy de la función sigmoide\n","from scipy.special import expit  \n","\n","# Se define una clase NeuronModel para implementar el perceptrón sigmoide\n","class NeuronModel():  \n","\n","    # Método para inicializar las instancias del modelo\n","    def __init__(self, X, y, learning_rate=0.01, error_threshold=0.001) -> None:  \n","        assert X.size != 0, \"X cannot be empty\"  # Verificamos que X no esté vacío\n","        assert y.size != 0, \"y cannot be empty\"  # Verificamos que y no esté vacío\n","        assert learning_rate > 0, \"learning rate must be positive\"  # Verificamos que la tasa de aprendizaje sea positiva\n","\n","        if not isinstance(X, np.ndarray):  # Convertimos X a un array de numpy si no lo es\n","            X = X.to_numpy()\n","        \n","        if not isinstance(y, np.ndarray):  # Convertimos y a un array de numpy si no lo es\n","            y = y.to_numpy().reshape(-1,1)\n","\n","        self.X = X  # Asignamos X a la instancia\n","        self.y = y  # Asignamos y a la instancia\n","        self.learning_rate = learning_rate  # Asignamos la tasa de aprendizaje\n","        self.w = np.zeros((X.shape[1], 1))  # Inicializamos los pesos con ceros\n","        self.b = np.zeros((1, 1))  # Inicializamos el bias con cero\n","        self.N = X.shape[0]  # Guardamos el número de ejemplos en el conjunto de datos\n","        self.J_iter = list()  # Inicializamos una lista para almacenar los valores de la función de costo\n","        self.stopping_tolerance = error_threshold  # Establecemos la tolerancia de parada para el entrenamiento\n","\n","    # Función para calcular la salida logística usando la función sigmoide\n","    def compute_logistic(self, w, b, X):  \n","        return expit(X @ w + b)  # Calculamos la combinación lineal de los inputs y aplicamos la función sigmoide\n","\n","    # Función para calcular la función de costo utilizando entropía cruzada\n","    def compute_cost(self, y_hat, y, N):  \n","        L = y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)  # Calculamos la entropía cruzada entre las predicciones y los valores reales\n","        J = -L.sum() / N  # Promediamos la entropía cruzada sobre todos los ejemplos\n","        return J  # Devolvemos el costo\n","\n","    # Función para realizar la propagación hacia adelante\n","    def forward_propagation(self, w, b, X):  \n","        a = self.compute_logistic(w, b, X)  # Calculamos la salida usando la función sigmoide\n","        return a  # Devolvemos la salida\n","\n","    # Función para calcular los gradientes (propagación hacia atrás)\n","    def backward_propagation(self, y_hat, y, w, X):  \n","        N = X.shape[0]  # Número de ejemplos\n","        gradient_w = np.multiply(y_hat - y, X).sum(axis=0) / N  # Calculamos el gradiente respecto a los pesos\n","        gradient_w = gradient_w.reshape(w.shape)  # Ajustamos la forma del gradiente para coincidir con los pesos\n","        gradient_b = (y_hat - y).sum() / N  # Calculamos el gradiente respecto al bias\n","        gradient_b = gradient_b.reshape(-1, 1)  # Ajustamos la forma del gradiente del bias\n","        return gradient_w, gradient_b  # Devolvemos los gradientes para los pesos y el bias\n","\n","    # Función para actualizar los parámetros del modelo\n","    def update_parameters(self, param, gradient):  \n","        return np.subtract(param, np.multiply(self.learning_rate, gradient))  # Actualizamos los parámetros restando el gradiente multiplicado por la tasa de aprendizaje\n","\n","    # Función para entrenar el modelo\n","    def train(self, verbose=0):  \n","\n","        y_hat = self.forward_propagation(self.w, self.b, self.X)  # Calculamos las predicciones iniciales\n","        J_prev = self.compute_cost(y_hat, self.y, self.N)  # Calculamos el costo inicial\n","        J, current_percentage_error = 0, 100  # Inicializamos el costo y el error porcentual\n","        w, b = self.w, self.b  # Inicializamos los pesos y el bias\n","\n","        while current_percentage_error > self.stopping_tolerance:  # Iteramos hasta que el error sea menor que la tolerancia\n","            J_prev = J  # Guardamos el costo anterior\n","\n","            y_hat = self.forward_propagation(w, b, self.X)  # Calculamos las nuevas predicciones\n","\n","            gradient_w, gradient_b = self.backward_propagation(y_hat, self.y, w, self.X)  # Calculamos los gradientes\n","\n","            w = self.update_parameters(w, gradient_w)  # Actualizamos los pesos\n","            b = self.update_parameters(b, gradient_b)  # Actualizamos el bias\n","\n","            y_hat = self.forward_propagation(w, b, self.X)  # Calculamos las predicciones actualizadas\n","            J = self.compute_cost(y_hat, self.y, self.N)  # Calculamos el nuevo costo\n","            self.J_iter.append(J)  # Almacenamos el costo para su posterior visualización\n","\n","            current_percentage_error = np.abs((J - J_prev) / J) * 100  # Calculamos el error porcentual entre el costo anterior y el actual\n","\n","            if verbose:  # Si verbose está activado, imprimimos los detalles del costo en cada iteración\n","                print(f\"J_previous: {J_prev:8.6f} \\t\"\n","                        f\"J_current: {J:8.6f} \\t\"\n","                        f\"Error: {current_percentage_error:8.6f}\")\n","\n","        self.w = w  # Guardamos los pesos finales\n","        self.b = b  # Guardamos el bias final\n","\n","    # Función para realizar predicciones con el modelo entrenado\n","    def predict(self, X):  \n","        a = self.forward_propagation(self.w, self.b, X)  # Calculamos las predicciones usando la propagación hacia adelante\n","        return a  # Devolvemos las predicciones\n","\n","    # Función para graficar la evolución del costo durante el entrenamiento\n","    def plot_cost_function(self):  \n","        plt.plot(self.J_iter)  # Graficamos los valores de la función de costo almacenados\n","        plt.xlabel('Iteration')  # Etiqueta para el eje x\n","        plt.ylabel(r\"$J_{\\mathbf{w}}$\")  # Etiqueta para el eje y\n","\n","    # Función para evaluar el modelo\n","    def evaluate(self, y_hat, y):  \n","\n","        if not isinstance(y, np.ndarray):  # Aseguramos que y esté en formato de array de numpy\n","            y = y.to_numpy().reshape(-1, 1)\n","\n","        y_hat[y_hat >= 0.5] = 1  # Convertimos las predicciones en 1 si son mayores o iguales a 0.5\n","        y_hat[y_hat < 0.5] = 0  # Convertimos las predicciones en 0 si son menores a 0.5\n","\n","        err = np.where((np.abs(y - y_hat) > 0) == True)[0]  # Calculamos los errores como las diferencias entre y_hat e y\n","\n","        return 1 - err.shape[0] / y.shape[0]  # Devolvemos la exactitud del modelo\n","\n","    # Función para obtener los parámetros del modelo (pesos y bias)\n","    def get_parameters(self):  \n","        return np.vstack((self.w, self.b))  # Concatenamos los pesos y el bias en una sola matriz y los devolvemos"]},{"cell_type":"markdown","metadata":{},"source":["## 11. Creación y Entrenamiento del Modelo\n","En este bloque se crea una instancia del modelo de perceptrón sigmoide, proporcionando el conjunto de entrenamiento, la tasa de aprendizaje y el umbral de error para detener el entrenamiento. Posteriormente, el modelo es entrenado, con `verbose=1` activado para mostrar el progreso del entrenamiento en cada iteración."]},{"cell_type":"markdown","metadata":{},"source":["**Salida**: Durante el entrenamiento, se imprimen detalles en cada iteración que incluyen el costo anterior, el costo actual, y el error porcentual de cambio. El modelo ajusta los pesos y el bias en cada iteración hasta que el error porcentual es menor que el umbral de error especificado (0.01). La salida muestra la progresión del costo a lo largo de las iteraciones."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11330,"status":"ok","timestamp":1726229552006,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"NCoQ267PQisx","outputId":"a0402f07-057c-4857-f4c2-12bb0ffd7cf8"},"outputs":[{"name":"stdout","output_type":"stream","text":["J_previous: 0.000000 \tJ_current: 0.434288 \tError: 100.000000\n","J_previous: 0.434288 \tJ_current: 0.337854 \tError: 28.542931\n","J_previous: 0.337854 \tJ_current: 0.284542 \tError: 18.736343\n","J_previous: 0.284542 \tJ_current: 0.249663 \tError: 13.970439\n","J_previous: 0.249663 \tJ_current: 0.224651 \tError: 11.133710\n","J_previous: 0.224651 \tJ_current: 0.205647 \tError: 9.240989\n","J_previous: 0.205647 \tJ_current: 0.190621 \tError: 7.882361\n","J_previous: 0.190621 \tJ_current: 0.178390 \tError: 6.856745\n","J_previous: 0.178390 \tJ_current: 0.168207 \tError: 6.053642\n","J_previous: 0.168207 \tJ_current: 0.159579 \tError: 5.407044\n","J_previous: 0.159579 \tJ_current: 0.152161 \tError: 4.874943\n","J_previous: 0.152161 \tJ_current: 0.145707 \tError: 4.429250\n","J_previous: 0.145707 \tJ_current: 0.140035 \tError: 4.050434\n","J_previous: 0.140035 \tJ_current: 0.135007 \tError: 3.724473\n","J_previous: 0.135007 \tJ_current: 0.130516 \tError: 3.441032\n","J_previous: 0.130516 \tJ_current: 0.126478 \tError: 3.192321\n","J_previous: 0.126478 \tJ_current: 0.122827 \tError: 2.972356\n","J_previous: 0.122827 \tJ_current: 0.119509 \tError: 2.776462\n","J_previous: 0.119509 \tJ_current: 0.116480 \tError: 2.600933\n","J_previous: 0.116480 \tJ_current: 0.113702 \tError: 2.442794\n","J_previous: 0.113702 \tJ_current: 0.111146 \tError: 2.299625\n","J_previous: 0.111146 \tJ_current: 0.108786 \tError: 2.169442\n","J_previous: 0.108786 \tJ_current: 0.106600 \tError: 2.050597\n","J_previous: 0.106600 \tJ_current: 0.104570 \tError: 1.941713\n","J_previous: 0.104570 \tJ_current: 0.102679 \tError: 1.841629\n","J_previous: 0.102679 \tJ_current: 0.100913 \tError: 1.749358\n","J_previous: 0.100913 \tJ_current: 0.099262 \tError: 1.664058\n","J_previous: 0.099262 \tJ_current: 0.097713 \tError: 1.585002\n","J_previous: 0.097713 \tJ_current: 0.096258 \tError: 1.511562\n","J_previous: 0.096258 \tJ_current: 0.094888 \tError: 1.443192\n","J_previous: 0.094888 \tJ_current: 0.093597 \tError: 1.379412\n","J_previous: 0.093597 \tJ_current: 0.092378 \tError: 1.319805\n","J_previous: 0.092378 \tJ_current: 0.091225 \tError: 1.263998\n","J_previous: 0.091225 \tJ_current: 0.090133 \tError: 1.211663\n","J_previous: 0.090133 \tJ_current: 0.089097 \tError: 1.162509\n","J_previous: 0.089097 \tJ_current: 0.088114 \tError: 1.116276\n","J_previous: 0.088114 \tJ_current: 0.087178 \tError: 1.072729\n","J_previous: 0.087178 \tJ_current: 0.086288 \tError: 1.031659\n","J_previous: 0.086288 \tJ_current: 0.085440 \tError: 0.992878\n","J_previous: 0.085440 \tJ_current: 0.084631 \tError: 0.956215\n","J_previous: 0.084631 \tJ_current: 0.083858 \tError: 0.921517\n","J_previous: 0.083858 \tJ_current: 0.083119 \tError: 0.888642\n","J_previous: 0.083119 \tJ_current: 0.082413 \tError: 0.857463\n","J_previous: 0.082413 \tJ_current: 0.081736 \tError: 0.827866\n","J_previous: 0.081736 \tJ_current: 0.081087 \tError: 0.799742\n","J_previous: 0.081087 \tJ_current: 0.080465 \tError: 0.772995\n","J_previous: 0.080465 \tJ_current: 0.079868 \tError: 0.747536\n","J_previous: 0.079868 \tJ_current: 0.079295 \tError: 0.723284\n","J_previous: 0.079295 \tJ_current: 0.078744 \tError: 0.700162\n","J_previous: 0.078744 \tJ_current: 0.078213 \tError: 0.678101\n","J_previous: 0.078213 \tJ_current: 0.077703 \tError: 0.657038\n","J_previous: 0.077703 \tJ_current: 0.077211 \tError: 0.636913\n","J_previous: 0.077211 \tJ_current: 0.076737 \tError: 0.617670\n","J_previous: 0.076737 \tJ_current: 0.076280 \tError: 0.599260\n","J_previous: 0.076280 \tJ_current: 0.075839 \tError: 0.581635\n","J_previous: 0.075839 \tJ_current: 0.075413 \tError: 0.564751\n","J_previous: 0.075413 \tJ_current: 0.075001 \tError: 0.548568\n","J_previous: 0.075001 \tJ_current: 0.074604 \tError: 0.533046\n","J_previous: 0.074604 \tJ_current: 0.074219 \tError: 0.518152\n","J_previous: 0.074219 \tJ_current: 0.073847 \tError: 0.503851\n","J_previous: 0.073847 \tJ_current: 0.073487 \tError: 0.490113\n","J_previous: 0.073487 \tJ_current: 0.073138 \tError: 0.476909\n","J_previous: 0.073138 \tJ_current: 0.072800 \tError: 0.464211\n","J_previous: 0.072800 \tJ_current: 0.072473 \tError: 0.451995\n","J_previous: 0.072473 \tJ_current: 0.072155 \tError: 0.440237\n","J_previous: 0.072155 \tJ_current: 0.071847 \tError: 0.428913\n","J_previous: 0.071847 \tJ_current: 0.071548 \tError: 0.418004\n","J_previous: 0.071548 \tJ_current: 0.071257 \tError: 0.407489\n","J_previous: 0.071257 \tJ_current: 0.070975 \tError: 0.397350\n","J_previous: 0.070975 \tJ_current: 0.070701 \tError: 0.387569\n","J_previous: 0.070701 \tJ_current: 0.070435 \tError: 0.378130\n","J_previous: 0.070435 \tJ_current: 0.070176 \tError: 0.369017\n","J_previous: 0.070176 \tJ_current: 0.069924 \tError: 0.360216\n","J_previous: 0.069924 \tJ_current: 0.069679 \tError: 0.351712\n","J_previous: 0.069679 \tJ_current: 0.069440 \tError: 0.343493\n","J_previous: 0.069440 \tJ_current: 0.069208 \tError: 0.335546\n","J_previous: 0.069208 \tJ_current: 0.068982 \tError: 0.327860\n","J_previous: 0.068982 \tJ_current: 0.068762 \tError: 0.320423\n","J_previous: 0.068762 \tJ_current: 0.068547 \tError: 0.313224\n","J_previous: 0.068547 \tJ_current: 0.068338 \tError: 0.306255\n","J_previous: 0.068338 \tJ_current: 0.068134 \tError: 0.299505\n","J_previous: 0.068134 \tJ_current: 0.067935 \tError: 0.292965\n","J_previous: 0.067935 \tJ_current: 0.067741 \tError: 0.286627\n","J_previous: 0.067741 \tJ_current: 0.067551 \tError: 0.280482\n","J_previous: 0.067551 \tJ_current: 0.067366 \tError: 0.274524\n","J_previous: 0.067366 \tJ_current: 0.067186 \tError: 0.268744\n","J_previous: 0.067186 \tJ_current: 0.067009 \tError: 0.263137\n","J_previous: 0.067009 \tJ_current: 0.066837 \tError: 0.257694\n","J_previous: 0.066837 \tJ_current: 0.066669 \tError: 0.252410\n","J_previous: 0.066669 \tJ_current: 0.066504 \tError: 0.247278\n","J_previous: 0.066504 \tJ_current: 0.066344 \tError: 0.242294\n","J_previous: 0.066344 \tJ_current: 0.066186 \tError: 0.237451\n","J_previous: 0.066186 \tJ_current: 0.066033 \tError: 0.232745\n","J_previous: 0.066033 \tJ_current: 0.065882 \tError: 0.228170\n","J_previous: 0.065882 \tJ_current: 0.065735 \tError: 0.223721\n","J_previous: 0.065735 \tJ_current: 0.065591 \tError: 0.219395\n","J_previous: 0.065591 \tJ_current: 0.065451 \tError: 0.215186\n","J_previous: 0.065451 \tJ_current: 0.065313 \tError: 0.211090\n","J_previous: 0.065313 \tJ_current: 0.065178 \tError: 0.207105\n","J_previous: 0.065178 \tJ_current: 0.065045 \tError: 0.203224\n","J_previous: 0.065045 \tJ_current: 0.064916 \tError: 0.199446\n","J_previous: 0.064916 \tJ_current: 0.064789 \tError: 0.195767\n","J_previous: 0.064789 \tJ_current: 0.064665 \tError: 0.192182\n","J_previous: 0.064665 \tJ_current: 0.064543 \tError: 0.188690\n","J_previous: 0.064543 \tJ_current: 0.064424 \tError: 0.185287\n","J_previous: 0.064424 \tJ_current: 0.064307 \tError: 0.181970\n","J_previous: 0.064307 \tJ_current: 0.064192 \tError: 0.178735\n","J_previous: 0.064192 \tJ_current: 0.064079 \tError: 0.175582\n","J_previous: 0.064079 \tJ_current: 0.063969 \tError: 0.172506\n","J_previous: 0.063969 \tJ_current: 0.063861 \tError: 0.169506\n","J_previous: 0.063861 \tJ_current: 0.063755 \tError: 0.166579\n","J_previous: 0.063755 \tJ_current: 0.063650 \tError: 0.163722\n","J_previous: 0.063650 \tJ_current: 0.063548 \tError: 0.160934\n","J_previous: 0.063548 \tJ_current: 0.063448 \tError: 0.158212\n","J_previous: 0.063448 \tJ_current: 0.063349 \tError: 0.155554\n","J_previous: 0.063349 \tJ_current: 0.063253 \tError: 0.152959\n","J_previous: 0.063253 \tJ_current: 0.063158 \tError: 0.150424\n","J_previous: 0.063158 \tJ_current: 0.063064 \tError: 0.147948\n","J_previous: 0.063064 \tJ_current: 0.062973 \tError: 0.145529\n","J_previous: 0.062973 \tJ_current: 0.062883 \tError: 0.143164\n","J_previous: 0.062883 \tJ_current: 0.062794 \tError: 0.140854\n","J_previous: 0.062794 \tJ_current: 0.062707 \tError: 0.138595\n","J_previous: 0.062707 \tJ_current: 0.062622 \tError: 0.136386\n","J_previous: 0.062622 \tJ_current: 0.062538 \tError: 0.134227\n","J_previous: 0.062538 \tJ_current: 0.062455 \tError: 0.132115\n","J_previous: 0.062455 \tJ_current: 0.062374 \tError: 0.130050\n","J_previous: 0.062374 \tJ_current: 0.062294 \tError: 0.128029\n","J_previous: 0.062294 \tJ_current: 0.062216 \tError: 0.126052\n","J_previous: 0.062216 \tJ_current: 0.062139 \tError: 0.124117\n","J_previous: 0.062139 \tJ_current: 0.062063 \tError: 0.122223\n","J_previous: 0.062063 \tJ_current: 0.061988 \tError: 0.120370\n","J_previous: 0.061988 \tJ_current: 0.061915 \tError: 0.118556\n","J_previous: 0.061915 \tJ_current: 0.061843 \tError: 0.116779\n","J_previous: 0.061843 \tJ_current: 0.061772 \tError: 0.115039\n","J_previous: 0.061772 \tJ_current: 0.061702 \tError: 0.113336\n","J_previous: 0.061702 \tJ_current: 0.061633 \tError: 0.111667\n","J_previous: 0.061633 \tJ_current: 0.061565 \tError: 0.110032\n","J_previous: 0.061565 \tJ_current: 0.061499 \tError: 0.108430\n","J_previous: 0.061499 \tJ_current: 0.061433 \tError: 0.106861\n","J_previous: 0.061433 \tJ_current: 0.061368 \tError: 0.105323\n","J_previous: 0.061368 \tJ_current: 0.061305 \tError: 0.103816\n","J_previous: 0.061305 \tJ_current: 0.061242 \tError: 0.102338\n","J_previous: 0.061242 \tJ_current: 0.061180 \tError: 0.100889\n","J_previous: 0.061180 \tJ_current: 0.061119 \tError: 0.099469\n","J_previous: 0.061119 \tJ_current: 0.061060 \tError: 0.098076\n","J_previous: 0.061060 \tJ_current: 0.061001 \tError: 0.096711\n","J_previous: 0.061001 \tJ_current: 0.060942 \tError: 0.095371\n","J_previous: 0.060942 \tJ_current: 0.060885 \tError: 0.094057\n","J_previous: 0.060885 \tJ_current: 0.060829 \tError: 0.092768\n","J_previous: 0.060829 \tJ_current: 0.060773 \tError: 0.091503\n","J_previous: 0.060773 \tJ_current: 0.060718 \tError: 0.090262\n","J_previous: 0.060718 \tJ_current: 0.060664 \tError: 0.089044\n","J_previous: 0.060664 \tJ_current: 0.060611 \tError: 0.087849\n","J_previous: 0.060611 \tJ_current: 0.060559 \tError: 0.086676\n","J_previous: 0.060559 \tJ_current: 0.060507 \tError: 0.085524\n","J_previous: 0.060507 \tJ_current: 0.060456 \tError: 0.084394\n","J_previous: 0.060456 \tJ_current: 0.060406 \tError: 0.083283\n","J_previous: 0.060406 \tJ_current: 0.060356 \tError: 0.082193\n","J_previous: 0.060356 \tJ_current: 0.060307 \tError: 0.081123\n","J_previous: 0.060307 \tJ_current: 0.060259 \tError: 0.080071\n","J_previous: 0.060259 \tJ_current: 0.060211 \tError: 0.079039\n","J_previous: 0.060211 \tJ_current: 0.060164 \tError: 0.078024\n","J_previous: 0.060164 \tJ_current: 0.060118 \tError: 0.077027\n","J_previous: 0.060118 \tJ_current: 0.060072 \tError: 0.076048\n","J_previous: 0.060072 \tJ_current: 0.060027 \tError: 0.075086\n","J_previous: 0.060027 \tJ_current: 0.059983 \tError: 0.074140\n","J_previous: 0.059983 \tJ_current: 0.059939 \tError: 0.073211\n","J_previous: 0.059939 \tJ_current: 0.059895 \tError: 0.072298\n","J_previous: 0.059895 \tJ_current: 0.059853 \tError: 0.071400\n","J_previous: 0.059853 \tJ_current: 0.059811 \tError: 0.070517\n","J_previous: 0.059811 \tJ_current: 0.059769 \tError: 0.069650\n","J_previous: 0.059769 \tJ_current: 0.059728 \tError: 0.068797\n","J_previous: 0.059728 \tJ_current: 0.059687 \tError: 0.067958\n","J_previous: 0.059687 \tJ_current: 0.059647 \tError: 0.067133\n","J_previous: 0.059647 \tJ_current: 0.059608 \tError: 0.066322\n","J_previous: 0.059608 \tJ_current: 0.059569 \tError: 0.065524\n","J_previous: 0.059569 \tJ_current: 0.059530 \tError: 0.064739\n","J_previous: 0.059530 \tJ_current: 0.059492 \tError: 0.063967\n","J_previous: 0.059492 \tJ_current: 0.059455 \tError: 0.063207\n","J_previous: 0.059455 \tJ_current: 0.059417 \tError: 0.062460\n","J_previous: 0.059417 \tJ_current: 0.059381 \tError: 0.061725\n","J_previous: 0.059381 \tJ_current: 0.059345 \tError: 0.061001\n","J_previous: 0.059345 \tJ_current: 0.059309 \tError: 0.060289\n","J_previous: 0.059309 \tJ_current: 0.059273 \tError: 0.059589\n","J_previous: 0.059273 \tJ_current: 0.059239 \tError: 0.058899\n","J_previous: 0.059239 \tJ_current: 0.059204 \tError: 0.058220\n","J_previous: 0.059204 \tJ_current: 0.059170 \tError: 0.057552\n","J_previous: 0.059170 \tJ_current: 0.059136 \tError: 0.056894\n","J_previous: 0.059136 \tJ_current: 0.059103 \tError: 0.056246\n","J_previous: 0.059103 \tJ_current: 0.059070 \tError: 0.055609\n","J_previous: 0.059070 \tJ_current: 0.059038 \tError: 0.054981\n","J_previous: 0.059038 \tJ_current: 0.059006 \tError: 0.054363\n","J_previous: 0.059006 \tJ_current: 0.058974 \tError: 0.053754\n","J_previous: 0.058974 \tJ_current: 0.058943 \tError: 0.053154\n","J_previous: 0.058943 \tJ_current: 0.058912 \tError: 0.052563\n","J_previous: 0.058912 \tJ_current: 0.058881 \tError: 0.051982\n","J_previous: 0.058881 \tJ_current: 0.058851 \tError: 0.051409\n","J_previous: 0.058851 \tJ_current: 0.058821 \tError: 0.050844\n","J_previous: 0.058821 \tJ_current: 0.058791 \tError: 0.050288\n","J_previous: 0.058791 \tJ_current: 0.058762 \tError: 0.049740\n","J_previous: 0.058762 \tJ_current: 0.058733 \tError: 0.049200\n","J_previous: 0.058733 \tJ_current: 0.058705 \tError: 0.048668\n","J_previous: 0.058705 \tJ_current: 0.058677 \tError: 0.048144\n","J_previous: 0.058677 \tJ_current: 0.058649 \tError: 0.047627\n","J_previous: 0.058649 \tJ_current: 0.058621 \tError: 0.047118\n","J_previous: 0.058621 \tJ_current: 0.058594 \tError: 0.046616\n","J_previous: 0.058594 \tJ_current: 0.058567 \tError: 0.046121\n","J_previous: 0.058567 \tJ_current: 0.058540 \tError: 0.045634\n","J_previous: 0.058540 \tJ_current: 0.058513 \tError: 0.045153\n","J_previous: 0.058513 \tJ_current: 0.058487 \tError: 0.044679\n","J_previous: 0.058487 \tJ_current: 0.058462 \tError: 0.044212\n","J_previous: 0.058462 \tJ_current: 0.058436 \tError: 0.043751\n","J_previous: 0.058436 \tJ_current: 0.058411 \tError: 0.043297\n","J_previous: 0.058411 \tJ_current: 0.058386 \tError: 0.042849\n","J_previous: 0.058386 \tJ_current: 0.058361 \tError: 0.042408\n","J_previous: 0.058361 \tJ_current: 0.058336 \tError: 0.041972\n","J_previous: 0.058336 \tJ_current: 0.058312 \tError: 0.041542\n","J_previous: 0.058312 \tJ_current: 0.058288 \tError: 0.041119\n","J_previous: 0.058288 \tJ_current: 0.058264 \tError: 0.040701\n","J_previous: 0.058264 \tJ_current: 0.058241 \tError: 0.040288\n","J_previous: 0.058241 \tJ_current: 0.058218 \tError: 0.039882\n","J_previous: 0.058218 \tJ_current: 0.058195 \tError: 0.039481\n","J_previous: 0.058195 \tJ_current: 0.058172 \tError: 0.039085\n","J_previous: 0.058172 \tJ_current: 0.058150 \tError: 0.038694\n","J_previous: 0.058150 \tJ_current: 0.058127 \tError: 0.038309\n","J_previous: 0.058127 \tJ_current: 0.058105 \tError: 0.037929\n","J_previous: 0.058105 \tJ_current: 0.058083 \tError: 0.037554\n","J_previous: 0.058083 \tJ_current: 0.058062 \tError: 0.037184\n","J_previous: 0.058062 \tJ_current: 0.058041 \tError: 0.036818\n","J_previous: 0.058041 \tJ_current: 0.058019 \tError: 0.036458\n","J_previous: 0.058019 \tJ_current: 0.057998 \tError: 0.036102\n","J_previous: 0.057998 \tJ_current: 0.057978 \tError: 0.035751\n","J_previous: 0.057978 \tJ_current: 0.057957 \tError: 0.035404\n","J_previous: 0.057957 \tJ_current: 0.057937 \tError: 0.035062\n","J_previous: 0.057937 \tJ_current: 0.057917 \tError: 0.034724\n","J_previous: 0.057917 \tJ_current: 0.057897 \tError: 0.034391\n","J_previous: 0.057897 \tJ_current: 0.057877 \tError: 0.034061\n","J_previous: 0.057877 \tJ_current: 0.057858 \tError: 0.033737\n","J_previous: 0.057858 \tJ_current: 0.057838 \tError: 0.033416\n","J_previous: 0.057838 \tJ_current: 0.057819 \tError: 0.033099\n","J_previous: 0.057819 \tJ_current: 0.057800 \tError: 0.032786\n","J_previous: 0.057800 \tJ_current: 0.057781 \tError: 0.032477\n","J_previous: 0.057781 \tJ_current: 0.057763 \tError: 0.032172\n","J_previous: 0.057763 \tJ_current: 0.057744 \tError: 0.031871\n","J_previous: 0.057744 \tJ_current: 0.057726 \tError: 0.031573\n","J_previous: 0.057726 \tJ_current: 0.057708 \tError: 0.031279\n","J_previous: 0.057708 \tJ_current: 0.057690 \tError: 0.030989\n","J_previous: 0.057690 \tJ_current: 0.057673 \tError: 0.030702\n","J_previous: 0.057673 \tJ_current: 0.057655 \tError: 0.030419\n","J_previous: 0.057655 \tJ_current: 0.057638 \tError: 0.030139\n","J_previous: 0.057638 \tJ_current: 0.057620 \tError: 0.029863\n","J_previous: 0.057620 \tJ_current: 0.057603 \tError: 0.029590\n","J_previous: 0.057603 \tJ_current: 0.057587 \tError: 0.029320\n","J_previous: 0.057587 \tJ_current: 0.057570 \tError: 0.029054\n","J_previous: 0.057570 \tJ_current: 0.057553 \tError: 0.028791\n","J_previous: 0.057553 \tJ_current: 0.057537 \tError: 0.028531\n","J_previous: 0.057537 \tJ_current: 0.057521 \tError: 0.028274\n","J_previous: 0.057521 \tJ_current: 0.057504 \tError: 0.028020\n","J_previous: 0.057504 \tJ_current: 0.057488 \tError: 0.027769\n","J_previous: 0.057488 \tJ_current: 0.057473 \tError: 0.027521\n","J_previous: 0.057473 \tJ_current: 0.057457 \tError: 0.027276\n","J_previous: 0.057457 \tJ_current: 0.057441 \tError: 0.027033\n","J_previous: 0.057441 \tJ_current: 0.057426 \tError: 0.026794\n","J_previous: 0.057426 \tJ_current: 0.057411 \tError: 0.026557\n","J_previous: 0.057411 \tJ_current: 0.057396 \tError: 0.026324\n","J_previous: 0.057396 \tJ_current: 0.057381 \tError: 0.026093\n","J_previous: 0.057381 \tJ_current: 0.057366 \tError: 0.025864\n","J_previous: 0.057366 \tJ_current: 0.057351 \tError: 0.025638\n","J_previous: 0.057351 \tJ_current: 0.057337 \tError: 0.025415\n","J_previous: 0.057337 \tJ_current: 0.057322 \tError: 0.025194\n","J_previous: 0.057322 \tJ_current: 0.057308 \tError: 0.024976\n","J_previous: 0.057308 \tJ_current: 0.057294 \tError: 0.024760\n","J_previous: 0.057294 \tJ_current: 0.057280 \tError: 0.024547\n","J_previous: 0.057280 \tJ_current: 0.057266 \tError: 0.024336\n","J_previous: 0.057266 \tJ_current: 0.057252 \tError: 0.024128\n","J_previous: 0.057252 \tJ_current: 0.057238 \tError: 0.023922\n","J_previous: 0.057238 \tJ_current: 0.057225 \tError: 0.023718\n","J_previous: 0.057225 \tJ_current: 0.057211 \tError: 0.023516\n","J_previous: 0.057211 \tJ_current: 0.057198 \tError: 0.023317\n","J_previous: 0.057198 \tJ_current: 0.057185 \tError: 0.023120\n","J_previous: 0.057185 \tJ_current: 0.057172 \tError: 0.022925\n","J_previous: 0.057172 \tJ_current: 0.057159 \tError: 0.022732\n","J_previous: 0.057159 \tJ_current: 0.057146 \tError: 0.022541\n","J_previous: 0.057146 \tJ_current: 0.057133 \tError: 0.022353\n","J_previous: 0.057133 \tJ_current: 0.057120 \tError: 0.022166\n","J_previous: 0.057120 \tJ_current: 0.057108 \tError: 0.021982\n","J_previous: 0.057108 \tJ_current: 0.057095 \tError: 0.021799\n","J_previous: 0.057095 \tJ_current: 0.057083 \tError: 0.021619\n","J_previous: 0.057083 \tJ_current: 0.057071 \tError: 0.021440\n","J_previous: 0.057071 \tJ_current: 0.057058 \tError: 0.021263\n","J_previous: 0.057058 \tJ_current: 0.057046 \tError: 0.021089\n","J_previous: 0.057046 \tJ_current: 0.057035 \tError: 0.020916\n","J_previous: 0.057035 \tJ_current: 0.057023 \tError: 0.020745\n","J_previous: 0.057023 \tJ_current: 0.057011 \tError: 0.020575\n","J_previous: 0.057011 \tJ_current: 0.056999 \tError: 0.020408\n","J_previous: 0.056999 \tJ_current: 0.056988 \tError: 0.020242\n","J_previous: 0.056988 \tJ_current: 0.056976 \tError: 0.020078\n","J_previous: 0.056976 \tJ_current: 0.056965 \tError: 0.019916\n","J_previous: 0.056965 \tJ_current: 0.056954 \tError: 0.019756\n","J_previous: 0.056954 \tJ_current: 0.056943 \tError: 0.019597\n","J_previous: 0.056943 \tJ_current: 0.056932 \tError: 0.019440\n","J_previous: 0.056932 \tJ_current: 0.056921 \tError: 0.019284\n","J_previous: 0.056921 \tJ_current: 0.056910 \tError: 0.019130\n","J_previous: 0.056910 \tJ_current: 0.056899 \tError: 0.018978\n","J_previous: 0.056899 \tJ_current: 0.056888 \tError: 0.018827\n","J_previous: 0.056888 \tJ_current: 0.056878 \tError: 0.018678\n","J_previous: 0.056878 \tJ_current: 0.056867 \tError: 0.018530\n","J_previous: 0.056867 \tJ_current: 0.056857 \tError: 0.018384\n","J_previous: 0.056857 \tJ_current: 0.056846 \tError: 0.018240\n","J_previous: 0.056846 \tJ_current: 0.056836 \tError: 0.018096\n","J_previous: 0.056836 \tJ_current: 0.056826 \tError: 0.017955\n","J_previous: 0.056826 \tJ_current: 0.056816 \tError: 0.017814\n","J_previous: 0.056816 \tJ_current: 0.056806 \tError: 0.017675\n","J_previous: 0.056806 \tJ_current: 0.056796 \tError: 0.017538\n","J_previous: 0.056796 \tJ_current: 0.056786 \tError: 0.017402\n","J_previous: 0.056786 \tJ_current: 0.056776 \tError: 0.017267\n","J_previous: 0.056776 \tJ_current: 0.056766 \tError: 0.017133\n","J_previous: 0.056766 \tJ_current: 0.056757 \tError: 0.017002\n","J_previous: 0.056757 \tJ_current: 0.056747 \tError: 0.016871\n","J_previous: 0.056747 \tJ_current: 0.056737 \tError: 0.016741\n","J_previous: 0.056737 \tJ_current: 0.056728 \tError: 0.016613\n","J_previous: 0.056728 \tJ_current: 0.056719 \tError: 0.016486\n","J_previous: 0.056719 \tJ_current: 0.056709 \tError: 0.016360\n","J_previous: 0.056709 \tJ_current: 0.056700 \tError: 0.016236\n","J_previous: 0.056700 \tJ_current: 0.056691 \tError: 0.016113\n","J_previous: 0.056691 \tJ_current: 0.056682 \tError: 0.015991\n","J_previous: 0.056682 \tJ_current: 0.056673 \tError: 0.015870\n","J_previous: 0.056673 \tJ_current: 0.056664 \tError: 0.015750\n","J_previous: 0.056664 \tJ_current: 0.056655 \tError: 0.015632\n","J_previous: 0.056655 \tJ_current: 0.056646 \tError: 0.015514\n","J_previous: 0.056646 \tJ_current: 0.056638 \tError: 0.015398\n","J_previous: 0.056638 \tJ_current: 0.056629 \tError: 0.015283\n","J_previous: 0.056629 \tJ_current: 0.056620 \tError: 0.015169\n","J_previous: 0.056620 \tJ_current: 0.056612 \tError: 0.015056\n","J_previous: 0.056612 \tJ_current: 0.056603 \tError: 0.014944\n","J_previous: 0.056603 \tJ_current: 0.056595 \tError: 0.014833\n","J_previous: 0.056595 \tJ_current: 0.056587 \tError: 0.014724\n","J_previous: 0.056587 \tJ_current: 0.056578 \tError: 0.014615\n","J_previous: 0.056578 \tJ_current: 0.056570 \tError: 0.014507\n","J_previous: 0.056570 \tJ_current: 0.056562 \tError: 0.014401\n","J_previous: 0.056562 \tJ_current: 0.056554 \tError: 0.014295\n","J_previous: 0.056554 \tJ_current: 0.056546 \tError: 0.014191\n","J_previous: 0.056546 \tJ_current: 0.056538 \tError: 0.014087\n","J_previous: 0.056538 \tJ_current: 0.056530 \tError: 0.013984\n","J_previous: 0.056530 \tJ_current: 0.056522 \tError: 0.013882\n","J_previous: 0.056522 \tJ_current: 0.056515 \tError: 0.013782\n","J_previous: 0.056515 \tJ_current: 0.056507 \tError: 0.013682\n","J_previous: 0.056507 \tJ_current: 0.056499 \tError: 0.013583\n","J_previous: 0.056499 \tJ_current: 0.056491 \tError: 0.013485\n","J_previous: 0.056491 \tJ_current: 0.056484 \tError: 0.013388\n","J_previous: 0.056484 \tJ_current: 0.056476 \tError: 0.013291\n","J_previous: 0.056476 \tJ_current: 0.056469 \tError: 0.013196\n","J_previous: 0.056469 \tJ_current: 0.056462 \tError: 0.013101\n","J_previous: 0.056462 \tJ_current: 0.056454 \tError: 0.013008\n","J_previous: 0.056454 \tJ_current: 0.056447 \tError: 0.012915\n","J_previous: 0.056447 \tJ_current: 0.056440 \tError: 0.012823\n","J_previous: 0.056440 \tJ_current: 0.056433 \tError: 0.012732\n","J_previous: 0.056433 \tJ_current: 0.056425 \tError: 0.012642\n","J_previous: 0.056425 \tJ_current: 0.056418 \tError: 0.012552\n","J_previous: 0.056418 \tJ_current: 0.056411 \tError: 0.012464\n","J_previous: 0.056411 \tJ_current: 0.056404 \tError: 0.012376\n","J_previous: 0.056404 \tJ_current: 0.056397 \tError: 0.012289\n","J_previous: 0.056397 \tJ_current: 0.056390 \tError: 0.012203\n","J_previous: 0.056390 \tJ_current: 0.056384 \tError: 0.012117\n","J_previous: 0.056384 \tJ_current: 0.056377 \tError: 0.012032\n","J_previous: 0.056377 \tJ_current: 0.056370 \tError: 0.011948\n","J_previous: 0.056370 \tJ_current: 0.056363 \tError: 0.011865\n","J_previous: 0.056363 \tJ_current: 0.056357 \tError: 0.011782\n","J_previous: 0.056357 \tJ_current: 0.056350 \tError: 0.011701\n","J_previous: 0.056350 \tJ_current: 0.056344 \tError: 0.011619\n","J_previous: 0.056344 \tJ_current: 0.056337 \tError: 0.011539\n","J_previous: 0.056337 \tJ_current: 0.056331 \tError: 0.011459\n","J_previous: 0.056331 \tJ_current: 0.056324 \tError: 0.011380\n","J_previous: 0.056324 \tJ_current: 0.056318 \tError: 0.011302\n","J_previous: 0.056318 \tJ_current: 0.056312 \tError: 0.011224\n","J_previous: 0.056312 \tJ_current: 0.056305 \tError: 0.011148\n","J_previous: 0.056305 \tJ_current: 0.056299 \tError: 0.011071\n","J_previous: 0.056299 \tJ_current: 0.056293 \tError: 0.010995\n","J_previous: 0.056293 \tJ_current: 0.056287 \tError: 0.010921\n","J_previous: 0.056287 \tJ_current: 0.056281 \tError: 0.010846\n","J_previous: 0.056281 \tJ_current: 0.056275 \tError: 0.010773\n","J_previous: 0.056275 \tJ_current: 0.056269 \tError: 0.010700\n","J_previous: 0.056269 \tJ_current: 0.056263 \tError: 0.010627\n","J_previous: 0.056263 \tJ_current: 0.056257 \tError: 0.010555\n","J_previous: 0.056257 \tJ_current: 0.056251 \tError: 0.010484\n","J_previous: 0.056251 \tJ_current: 0.056245 \tError: 0.010414\n","J_previous: 0.056245 \tJ_current: 0.056239 \tError: 0.010344\n","J_previous: 0.056239 \tJ_current: 0.056233 \tError: 0.010274\n","J_previous: 0.056233 \tJ_current: 0.056228 \tError: 0.010206\n","J_previous: 0.056228 \tJ_current: 0.056222 \tError: 0.010137\n","J_previous: 0.056222 \tJ_current: 0.056216 \tError: 0.010069\n","J_previous: 0.056216 \tJ_current: 0.056211 \tError: 0.010003\n","J_previous: 0.056211 \tJ_current: 0.056205 \tError: 0.009936\n"]}],"source":["# Creamos una instancia de NeuronModel, pasando las características y etiquetas de entrenamiento, una tasa de aprendizaje de 1 y un umbral de error de 0.01\n","model = NeuronModel(X=X_train, y=y_train, learning_rate=1, error_threshold=0.01)\n","\n","# Entrenamos el modelo con el conjunto de entrenamiento y activamos verbose=1 para imprimir detalles en cada iteración\n","model.train(verbose=1)"]},{"cell_type":"markdown","metadata":{},"source":["## 12. Visualización de la Función de Costo\n","Este bloque grafica la función de costo a lo largo de las iteraciones del entrenamiento. Nos permite observar cómo el costo disminuye con el tiempo, lo cual es un indicador de que el modelo está aprendiendo y ajustando sus parámetros correctamente."]},{"cell_type":"markdown","metadata":{},"source":["**Salida**: Se genera un gráfico que muestra la evolución de la función de costo a lo largo de las iteraciones del entrenamiento. El eje x representa las iteraciones, y el eje y muestra el valor del costo. Este gráfico permite visualizar cómo el costo disminuye a medida que el modelo se entrena, indicando que el modelo está aprendiendo."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"executionInfo":{"elapsed":785,"status":"ok","timestamp":1726229560744,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"SW9hZDqqQisx","outputId":"aaa5c50f-e86b-47cc-8fdd-f061b509ae67"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Graficamos la evolución de la función de costo durante las iteraciones del entrenamiento\n","model.plot_cost_function()"]},{"cell_type":"markdown","metadata":{},"source":["## 13. Realización de Predicciones\n","En este bloque se utilizan los pesos y bias del modelo entrenado para realizar predicciones sobre el conjunto de prueba (`X_test`). El modelo aplica la propagación hacia adelante para generar las predicciones."]},{"cell_type":"markdown","metadata":{},"source":["**Salida**: No hay salida directa. El modelo entrenado realiza predicciones en el conjunto de prueba (`X_test`), devolviendo las etiquetas predichas almacenadas en `y_predictions`, que se utilizarán para evaluar el rendimiento del modelo en el conjunto de prueba."]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":289,"status":"ok","timestamp":1726229564958,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"Xi1dSaZBQisx"},"outputs":[],"source":["# Usamos el modelo entrenado para predecir las etiquetas del conjunto de prueba\n","y_predictions = model.predict(X=X_test)"]},{"cell_type":"markdown","metadata":{},"source":["## 14. Obtención de los Parámetros del Modelo\n","Este bloque devuelve los parámetros del modelo, es decir, los pesos y el bias, que fueron aprendidos durante el proceso de entrenamiento. Estos parámetros determinan cómo el modelo toma decisiones y realiza las predicciones."]},{"cell_type":"markdown","metadata":{},"source":["**Salida**: Se muestra una matriz que contiene los parámetros aprendidos por el modelo durante el entrenamiento. Esta matriz incluye los pesos asociados a cada característica y el valor del bias. Estos valores representan el ajuste final del modelo para realizar las predicciones."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":424,"status":"ok","timestamp":1726229566800,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"gr9mNSCzQisx","outputId":"6d3a869d-9344-4cf6-9629-e0c073658b7f"},"outputs":[{"data":{"text/plain":["array([[-0.72815137],\n","       [ 0.20614421],\n","       [ 4.29723527],\n","       [ 0.95814606],\n","       [ 0.17301175],\n","       [-3.70904074]])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Obtenemos los parámetros (pesos y bias) aprendidos por el modelo durante el entrenamiento\n","model.get_parameters()"]},{"cell_type":"markdown","metadata":{},"source":["## 15. Evaluación del Modelo\n","Este bloque evalúa el rendimiento del modelo utilizando las predicciones generadas (`y_predictions`) y las etiquetas reales del conjunto de prueba (`y_test`). La función devuelve la exactitud del modelo, que indica qué tan bien predice el modelo en comparación con los valores reales."]},{"cell_type":"markdown","metadata":{},"source":["**Salida**: Se devuelve y muestra la precisión del modelo. Esta métrica indica el porcentaje de predicciones correctas hechas por el modelo en el conjunto de prueba, comparando las predicciones (`y_predictions`) con las etiquetas reales (`y_test`). Una precisión alta indica un buen rendimiento del modelo. "]},{"cell_type":"code","execution_count":16,"metadata":{"id":"A22D2s2PQisx","outputId":"60daca34-5f6a-429b-f0f3-9e20250f2af3"},"outputs":[{"data":{"text/plain":["0.9892996108949417"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Evaluamos la precisión del modelo comparando las predicciones (y_predictions) con las etiquetas reales (y_test)\n","model.evaluate(y_hat=y_predictions, y=y_test)"]},{"cell_type":"markdown","metadata":{},"source":["## 16. Cálculo de Métricas de Rendimiento del Clasificador Perceptrón\n","En este bloque se calcula la precisión, precisión (precision), recall, especificidad y F1-score para evaluar el rendimiento del clasificador perceptrón en el conjunto de prueba. Estas métricas permiten un análisis más profundo de la capacidad del modelo para clasificar correctamente las instancias positivas y negativas, evaluando también su capacidad para evitar falsos positivos y falsos negativos."]},{"cell_type":"markdown","metadata":{},"source":["**Salida**: Se imprimen varias métricas de rendimiento basadas en la matriz de confusión:\n","- **Accuracy (Precisión global)**: Mide la proporción de predicciones correctas sobre el total de predicciones.\n","- **Precision (Precisión)**: Mide cuántas de las predicciones positivas realizadas por el modelo fueron correctas.\n","- **Recall (Sensibilidad)**: Mide cuántas de las instancias positivas reales fueron correctamente identificadas por el modelo.\n","- **Specificity (Especificidad)**: Mide cuántas de las instancias negativas reales fueron correctamente identificadas.\n","- **F1-score**: Es la media armónica entre la precisión y el recall, proporcionando una métrica equilibrada entre ambas.\n","<br><br>\n","Estas métricas permiten una evaluación completa del rendimiento del modelo."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.989300\n","Precision: 0.957645\n","Recall: 0.996774\n","Specificity: 0.987115\n","F1-score: 0.976818\n"]}],"source":["# Importamos la función confusion_matrix de sklearn para calcular la matriz de confusión\n","from sklearn.metrics import confusion_matrix\n","\n","# Hacemos predicciones en el conjunto de prueba usando el modelo entrenado\n","# Convertimos las predicciones en valores binarios, clasificando como 1 si el valor es mayor a 0.5\n","y_predictions = model.predict(X_test) > 0.5  \n","\n","# Calculamos la matriz de confusión comparando las etiquetas reales y las predicciones del modelo\n","c_matrix = confusion_matrix(y_true=y_test, y_pred=y_predictions)\n","\n","# Definimos una función para calcular varias métricas de rendimiento a partir de la matriz de confusión\n","def get_performance_metrics(c_matrix):\n","    # Extraemos los valores de la matriz de confusión: verdaderos negativos (tn), falsos positivos (fp),\n","    # falsos negativos (fn), y verdaderos positivos (tp)\n","    tn, fp, fn, tp = c_matrix[0][0], c_matrix[0][1], c_matrix[1][0], c_matrix[1][1]\n","    \n","    # Calculamos la precisión global (accuracy), que mide la proporción de predicciones correctas\n","    accuracy = (tp + tn) / (tp + tn + fp + fn)\n","    \n","    # Calculamos la precisión (precision), que mide la proporción de predicciones positivas correctas\n","    precision = tp / (tp + fp)\n","    \n","    # Calculamos el recall (sensibilidad), que mide la proporción de verdaderos positivos detectados\n","    recall = tp / (tp + fn)\n","    \n","    # Calculamos la especificidad, que mide la proporción de verdaderos negativos correctamente clasificados\n","    specificity = tn / (tn + fp)\n","    \n","    # Calculamos el F1-score, que es la media armónica entre precisión y recall\n","    f1_score = 2 * (precision * recall) / (precision + recall)\n","    \n","    # Devolvemos todas las métricas calculadas\n","    return accuracy, precision, recall, specificity, f1_score\n","\n","# Llamamos a la función para calcular las métricas de rendimiento utilizando la matriz de confusión\n","accuracy, precision, recall, specificity, f1_score = get_performance_metrics(c_matrix)\n","\n","# Imprimimos la precisión global con 6 decimales\n","print(f\"Accuracy: {accuracy:0.6f}\")\n","# Imprimimos la precisión con 6 decimales\n","print(f\"Precision: {precision:0.6f}\")\n","# Imprimimos el recall con 6 decimales\n","print(f\"Recall: {recall:0.6f}\")\n","# Imprimimos la especificidad con 6 decimales\n","print(f\"Specificity: {specificity:0.6f}\")\n","# Imprimimos el F1-score con 6 decimales\n","print(f\"F1-score: {f1_score:0.6f}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Explicación de las Métricas de Rendimiento: Accuracy, Precision, Recall, Specificity y F1-score\n","- **Accuracy (Precisión global)**: La precisión mide la proporción de predicciones correctas en el conjunto de prueba. Si bien es una métrica útil, puede ser engañosa en conjuntos de datos desbalanceados, ya que un modelo podría obtener una alta precisión simplemente prediciendo la clase mayoritaria. En este caso, una precisión alta indica que el modelo está clasificando correctamente la mayoría de las instancias.\n","- **Precision (Precisión)**: La precisión es la proporción de verdaderos positivos entre todas las predicciones positivas realizadas por el modelo. Es una métrica importante cuando el costo de tener falsos positivos es alto. Una precisión alta significa que el modelo es confiable cuando predice una clase positiva (en este caso, la ocupación).\n","- **Recall (Sensibilidad)**: El recall mide la proporción de verdaderos positivos correctamente identificados entre todas las instancias positivas reales. Es útil en situaciones donde es más importante minimizar los falsos negativos. Una alta sensibilidad indica que el modelo detecta la mayoría de las instancias positivas.\n","- **Specificity (Especificidad)**: La especificidad mide la proporción de verdaderos negativos correctamente identificados entre todas las instancias negativas reales. Esta métrica es relevante cuando es importante evitar los falsos positivos. Una alta especificidad indica que el modelo es bueno detectando las instancias negativas (habitaciones no ocupadas).\n","- **F1-score**: El F1-score es la media armónica entre la precisión y el recall, y es útil cuando hay un balance entre la necesidad de minimizar tanto los falsos positivos como los falsos negativos. Un F1-score alto indica que el modelo tiene un buen equilibrio entre precisión y sensibilidad, siendo particularmente útil cuando las clases están desbalanceadas. <br><br>\n","Estas métricas permiten una evaluación más completa del rendimiento del modelo en lugar de confiar únicamente en la precisión global, lo que puede ser engañoso en ciertos escenarios (como cuando las clases están desvalanceadas o cuando se busca tener menos falsos negativos).\n"]},{"cell_type":"markdown","metadata":{},"source":["### Análisis de Resultados:\n","\n","1. **Accuracy (0.989300)**: El modelo tiene una precisión global muy alta, con aproximadamente un 98.93% de predicciones correctas. Esto indica que el modelo es muy eficaz en clasificar correctamente tanto las instancias positivas como negativas.\n","\n","2. **Precision (0.957645)**: La precisión es del 95.76%, lo que significa que, de todas las predicciones positivas que hizo el modelo, un 95.76% fueron correctas. Esto es importante en situaciones donde los falsos positivos deben minimizarse, como en la detección de ocupación incorrecta.\n","\n","3. **Recall (0.996774)**: El recall es extremadamente alto, con un valor de 99.68%. Esto indica que el modelo es muy bueno detectando las instancias positivas reales (ocupación), lo que significa que muy pocos casos positivos fueron clasificados incorrectamente como negativos (falsos negativos).\n","\n","4. **Specificity (0.987115)**: La especificidad es del 98.71%, lo que indica que el modelo es también muy eficaz para identificar correctamente las instancias negativas (habitaciones no ocupadas). Este resultado sugiere que el modelo evita con éxito los falsos positivos.\n","\n","5. **F1-score (0.976818)**: El F1-score, que combina la precisión y el recall, es de 97.68%, lo que indica un equilibrio sólido entre ambas métricas. El modelo mantiene un buen balance entre minimizar los falsos positivos y los falsos negativos, lo que es ideal en problemas de clasificación con una mezcla de métricas críticas.\n","\n","### Conclusión:\n","Los resultados muestran que el modelo entrenado es altamente efectivo en la clasificación binaria de ocupación, con un equilibrio saludable entre precisión, recall y especificidad. Esto sugiere que el modelo es capaz de hacer predicciones muy precisas en general, tanto identificando correctamente los positivos (ocupación) como los negativos (no ocupación), minimizando al mismo tiempo los falsos positivos y los falsos negativos.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Selección del Mejor Modelo\n","### Comparación de los Modelos A (Hecho a Mano) y B (TensorFlow)\n","#### Parámetros Obtenidos:\n","\n","**Modelo A (Hecho a Mano)**:\n","Los parámetros (pesos y bias) obtenidos manualmente son los siguientes: <br>\n","array([[-0.72815137], [ 0.20614421], [ 4.29723527], [ 0.95814606], [ 0.17301175], [-3.70904074]])\n","\n","**Modelo B (TensorFlow)**:\n","Los parámetros obtenidos utilizando TensorFlow son los siguientes: <br>\n","array([[-0.6764667 ], [ 0.48139647], [ 4.514354 ], [ 1.0179557 ], [-0.09119653]], dtype=float32), array([-3.9561274], dtype=float32)\n","\n","\n","#### Análisis de los Parámetros:\n","\n","1. **Pesos**:\n","   - Los pesos entre ambos modelos son similares en magnitud, pero con ligeras diferencias. Estas diferencias pueden atribuirse a las optimizaciones que aplica TensorFlow, como el uso del optimizador Adam, que ajusta dinámicamente la tasa de aprendizaje.\n","   \n","   - **Comparación Peso por Peso**:\n","     - El primer peso es ligeramente menor en el modelo B (`-0.6764667`) comparado con el modelo A (`-0.72815137`), lo que puede indicar un ajuste más fino en el modelo de TensorFlow.\n","     - El segundo peso en el modelo B (`0.48139647`) es mayor que en el modelo A (`0.20614421`), lo que sugiere que el modelo B asigna mayor importancia a esta característica.\n","     - El tercer peso es bastante similar en ambos modelos, pero el modelo B asigna un valor ligeramente mayor (`4.514354` vs `4.29723527`).\n","     - El cuarto peso es un poco mayor en el modelo B (`1.0179557` vs `0.95814606`).\n","     - El quinto peso es positivo en el modelo A (`0.17301175`), pero negativo en el modelo B (`-0.09119653`), lo que refleja un enfoque diferente en cómo cada modelo interpreta esta característica.\n","\n","   - **Bias**:\n","     - El bias del modelo A es `-3.70904074`, mientras que en el modelo B es `-3.9561274`. Aunque ambos son similares, el ajuste más negativo del bias en el modelo B puede contribuir a su mejor rendimiento.\n","\n","   - **Conclusión sobre los Pesos**:\n","     - El modelo B ajusta los pesos de manera más precisa gracias a las optimizaciones de TensorFlow, lo que le permite asignar mejor la importancia a cada característica y lograr un rendimiento ligeramente superior en las métricas de evaluación.\n","\n","#### Diferencias en la Implementación:\n","\n","- **Modelo A (Hecho a Mano)**:\n","  - Este modelo implementa un perceptrón sigmoide de forma manual, calculando la función de costo, los gradientes, y actualizando los pesos mediante gradiente descendente.\n","  - Si bien es funcional, las pequeñas diferencias en los pesos y bias indican que no está tan optimizado como el modelo de TensorFlow.\n","  - El aprendizaje de este modelo puede ser más lento o menos eficiente en situaciones con grandes volúmenes de datos.\n","\n","- **Modelo B (TensorFlow)**:\n","  - Utiliza TensorFlow y aplica optimizaciones avanzadas, como el optimizador Adam, que ajusta dinámicamente la tasa de aprendizaje durante el entrenamiento.\n","  - Estas optimizaciones permiten obtener un ajuste más preciso de los parámetros (pesos y bias) y mejorar el rendimiento general del modelo.\n","  - Este modelo es ideal cuando se requiere entrenar con conjuntos de datos grandes o cuando se desea obtener resultados de manera más rápida y eficiente.\n","\n","#### Métricas de Rendimiento:\n","\n","| Métrica       | Modelo A (Hecho a Mano) | Modelo B (TensorFlow) |\n","|---------------|-------------------------|-----------------------|\n","| **Accuracy**  | 0.989300                | 0.989543              |\n","| **Precision** | 0.957645                | 0.958635              |\n","| **Recall**    | 0.996774                | 0.996774              |\n","| **Specificity**| 0.987115               | 0.987429              |\n","| **F1-score**  | 0.976818                | 0.977333              |\n","\n","- **Accuracy**: Ambos modelos tienen una precisión muy alta, pero el modelo B (TensorFlow) tiene una ligera ventaja.\n","- **Precision**: El modelo B tiene una precisión ligeramente mejor, lo que significa que comete menos falsos positivos.\n","- **Recall**: Ambos modelos tienen el mismo recall, lo que indica que ambos identifican correctamente casi todas las instancias positivas.\n","- **Specificity**: El modelo B también tiene una ventaja en la especificidad, identificando mejor las instancias negativas.\n","- **F1-score**: El modelo B presenta un F1-score mejorado, lo que sugiere un mejor equilibrio entre la precisión y el recall.\n","\n","#### Conclusión:\n","\n","El **Modelo B (TensorFlow)** es el mejor debido a sus ventajas en términos de:\n","- **Optimización interna**: TensorFlow utiliza técnicas avanzadas de ajuste de parámetros como el optimizador Adam, lo que mejora la tasa de aprendizaje y ajuste de los pesos.\n","- **Eficiencia**: TensorFlow es más eficiente en términos de tiempo de cómputo y optimización de recursos, lo que es crucial para manejar grandes cantidades de datos.\n","- **Rendimiento**: Aunque las diferencias en las métricas son pequeñas, el modelo B supera al modelo A en todas las métricas principales, lo que lo hace una mejor opción en términos de precisión y ajuste del modelo.\n","\n","En conclusión, el **Modelo B (TensorFlow)** es la mejor opción; ya que, ofrece un rendimiento ligeramente superior y está mejor optimizado para un entrenamiento más eficiente y preciso."]},{"cell_type":"markdown","metadata":{},"source":["**Código de Honor** <br>\n","*\"Doy mi palabra de que he realizado esta actividad con Integridad Académica\"*"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"ai","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
